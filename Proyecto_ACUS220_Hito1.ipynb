{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047d525e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"figuras/escudo_uach.png\" alt=\"UACh\" width=\"200\"/>\n",
    "    </p>\n",
    "\n",
    "Clasificador de instrumentos: Adivina qué\n",
    "\n",
    "### Primer Hito de Entrega  \n",
    "\n",
    "**Integrantes:** Katherine Zapata, Benjamín Martinez.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce9095",
   "metadata": {},
   "source": [
    "## Objetivos del Proyecto  \n",
    "\n",
    "**Objetivo General:**  \n",
    "- Desarrollar una aplicación interactiva capaz de grabar un audio en tiempo real, hacer un análisis espectral y determinar el instrumento musical con mayor similitud entre una base de datos preexistentes, utilizando técnicas de procesamiento digital de señales y posibles métodos de aprendizaje automático.\n",
    "\n",
    "**Objetivos Específicos:**  \n",
    "- OE1: Preparar uan base datos de sonidos de distintos intrumentos musicales, priorizando el correcto etiquetado de las muestras y una  correcta organización.\n",
    "  \n",
    "- OE2: Implementar funciones de grabación y procesamiento de audio en tiempo real, permitiendo la captura y almacenamiento temporal de sonidos del usuario.\n",
    "\n",
    "- OE3: Analizar y comparar las características espectrales (espectro, centroides, etc.) entre los sonidos grabados y las muestras de referencia mediante técnicas como la Transformada Rápida de Fourier (FFT) y métricas de similitud.\n",
    "\n",
    "- OE4: Visualizar los resultados mediante gráficos comparativos y mostrar la identificación del instrumento más similar, acompañada de la correspondiente imagen de referencia.\n",
    "\n",
    "- OE5: Integrar y probar un modelo de clasificación automático simple(por ejemplo, k-Nearest Neighbors), entrenado con características acústicas extraídas, para mejorar la precisión de identificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba83cb",
   "metadata": {},
   "source": [
    "## Estado del Arte  \n",
    " El reconocimiento automático de sonidos y, en particular, de instrumentos musicales, es un campo activo dentro del procesamiento digital de señales (DSP) y el aprendizaje automático aplicado al audio.\n",
    " El análisis de frecuencia mediante la Transformada Rápida de Fourier (FFT) ha sido históricamente una de las técnicas más utilizadas para representar las características espectrales de un sonido, permitiendo identificar patrones que distinguen diferentes fuentes sonoras.\n",
    "Actualmente, herramientas y librerías como Librosa, SciPy y NumPy han facilitado la implementación de métodos de análisis espectral y extracción de características acústicas (como MFCC, centroides espectrales o roll-off de frecuencia), que son la base para tareas de clasificación y comparación de sonidos.\n",
    "Estos enfoques son ampliamente utilizados en aplicaciones como:\n",
    "- La clasificación de géneros musicales y detección de instrumentos.\n",
    "- La identificación de especies animales a partir de grabaciones ambientales.\n",
    "- La detección de eventos acústicos en entornos urbanos o industriales.\n",
    "\n",
    "En los últimos años, los modelos de aprendizaje automático y redes neuronales han mejorado la precisión de estos sistemas, pero muchas de esas soluciones requieren grandes volúmenes de datos y potencia de cómputo considerable.\n",
    "Este proyecto busca un enfoque didáctico y accesible, que permita comprender los fundamentos del reconocimiento de sonidos sin depender de modelos complejos. Aun así, se propone incluir un modelo de clasificación simple (como KNN o SVM) para demostrar cómo la inteligencia artificial puede complementar el análisis espectral tradicional.\n",
    "El interés principal del proyecto radica en su carácter educativo e interactivo: permite explorar conceptos de acústica computacional, análisis de frecuencia y reconocimiento de patrones de forma visual y experimental.\n",
    "Además, su implementación en Python y Jupyter Notebook lo convierte en una herramienta abierta y reproducible, ideal para estudiantes o aficionados que deseen aprender sobre procesamiento de audio y machine learning aplicado al sonido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc17aa5",
   "metadata": {},
   "source": [
    "## Materiales y Métodos  \n",
    "\n",
    "### Materiales  \n",
    "- Base de datos de sonidos: Archivos de audio .wav de instrumentos musicales (ej: piano, guitarra, flauta, etc.).\n",
    "- Imágenes de referencia: Archivos .png o .jpg representativos de cada instrumento.\n",
    "- Micrófono: Para la grabación de sonidos en tiempo real por los usuarios.\n",
    "- Computador personal con entorno Anaconda instalado.\n",
    "- Librerías de Python:\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - librosa\n",
    "    - scipy\n",
    "    - sounddevice\n",
    "    - pillow (para imágenes)\n",
    "- Jupyter Notebook: Para el desarrollo y ejecución interactiva del sistema.\n",
    "\n",
    "### Metodología  \n",
    "El proyecto consiste en desarrollar una aplicación interactiva en Python que permita grabar un sonido en tiempo real, analizar sus características espectrales y determinar qué instrumento musical es más similar entre una base de sonidos preexistentes.\n",
    "Para ello, se implementará un flujo de trabajo compuesto por las siguientes etapas:\n",
    "\n",
    "1.\tPreparación de la base de datos de sonidos\n",
    " Se reunirán muestras cortas de instrumentos musicales (archivos .wav), idealmente grabadas en condiciones controladas o descargadas de librerías abiertas. Cada muestra será etiquetada con el nombre del instrumento correspondiente.\n",
    "\n",
    "2.\tGrabación de nuevos sonidos mediante micrófono\n",
    " El sistema permitirá capturar sonidos directamente desde el micrófono del usuario, almacenando temporalmente para su posterior análisis.\n",
    "\n",
    "3.\tAnálisis espectral y variaciones de frecuencia\n",
    " Se aplicará la Transformada Rápida de Fourier (FFT) para obtener el espectro de frecuencias de cada sonido. Además, se calcularán espectrogramas y variaciones de frecuencia en el tiempo, permitiendo capturar patrones temporales característicos de cada instrumento (por ejemplo, vibrato, ataques o decaimientos del sonido).\n",
    "\n",
    "4.\tCálculo de métricas de similitud\n",
    " Los espectros y espectrogramas serán comparados entre sí mediante distintas métricas, como la distancia euclidiana, la correlación cruzada o la distancia coseno, para determinar qué muestra de la base de datos es más similar al sonido grabado.\n",
    "\n",
    "5.\tImplementación opcional de un modelo de IA\n",
    " Para mejorar la precisión del reconocimiento, se evaluará el uso de un modelo de aprendizaje automático liviano (por ejemplo, k-Nearest Neighbors o un clasificador SVM) entrenado con las características extraídas (como centroides espectrales, roll-off, MFCCs, etc.).\n",
    " Esto permitirá que el sistema aprenda a distinguir instrumentos a partir de sus patrones de frecuencia sin depender únicamente de comparaciones directas.\n",
    "\n",
    "6.\tInterfaz interactiva en Jupyter Notebook\n",
    " El usuario podrá ejecutar el sistema desde un entorno Jupyter Notebook, donde podrá:\n",
    "\n",
    "- Grabar sonidos directamente.\n",
    "- Visualizar los espectros y espectrogramas.\n",
    "- Escuchar los audios comparados.\n",
    "- Ver el instrumento identificado junto con una gráfica comparativa de similitud.\n",
    "\n",
    "**Plan de trabajo:**Tentativo\n",
    "\n",
    "| Actividad                                          | Responsable(s)  | Fecha estimada |\n",
    "|----------------------------------------------------|-----------------|----------------|\n",
    "| Revisión bibliográfica y estado del arte           | Grupo completo  | 06/10/2025     |\n",
    "| Creación de la base de datos de sonidos            | Katherine       | 10/10/2025     |\n",
    "| Recolección y organización de imágenes             | Benjamin        | 14/10/2025     |\n",
    "| Desarrollo del sistema de grabación en Python\t     | Benjamin\t       | 22/10/2025     |\n",
    "| Implementación de análisis espectral y métricas    | Katherine       | 22/10/2025     |\n",
    "| Desarrollo de interfaz y visualizaciones\t         | Grupo Completo  | 04/11/2025     |\n",
    "| Entrenamiento de modelo IA simple\t        \t     | Grupo completo  | 10/11/2025     |\n",
    "| Pruebas funcionales y ajuste de parámetros         | Grupo Completo  | 12/11/2025     |\n",
    "| Redacción de informe final y entrega\t             | Grupo completo  | 20/11/2025     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb4b2e",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas  \n",
    "\n",
    "- McFee, B., Raffel, C., Liang, D., Ellis, D. P., McVicar, M., Battenberg, E., & Nieto, O. (2015). librosa: Audio and music signal analysis in Python. Proceedings of the 14th Python in Science Conference, 18–24. https://doi.org/10.25080/Majora-7b98e3ed-003\n",
    "\n",
    "- Virtanen, P., Gommers, R., Oliphant, T. E., et al. (2020). SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature Methods, 17(3), 261–272. https://doi.org/10.1038/s41592-019-0686-2\n",
    "\n",
    "- Rabiner, L. R., & Schafer, R. W. (2011). Theory and applications of digital speech processing. Pearson Higher Ed.\n",
    "\n",
    "- Lyons, R. G. (2010). Understanding Digital Signal Processing (3rd ed.). Prentice Hall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976b888-1626-4d5a-bda1-70e82a47c291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_k",
   "language": "python",
   "name": "entorno_k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
